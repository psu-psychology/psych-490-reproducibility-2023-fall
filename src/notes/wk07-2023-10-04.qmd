---
title: "2023-10-04 Wed"
subtitle: "Questionable research practices"
author: "Rick Gilmore"
format: revealjs
bibliography: [../include/bib/packages.bib, ../include/bib/psu-repro.bib]
csl: ../include/bib/apa.csl
css: ../include/css/styles.css
footer: "[PSYCH 490.009](../index.html): 2023-10-04 Wed"
---

# Overview

## In the news...

![[@Anthony_Mills2023-ji]](../include/img/mills-nytimes-americans-losing-trust-in-sci.png){fig-align="center"}

## Announcements

- [Due Friday]{.orange_due}
    - [Exercise 04: P-hack your way to scientific glory](../exercises/ex04-p-hacking.qmd) write-up.
- Discuss Friday [Exercise 04: P-hack your way to scientific glory](../exercises/ex04-p-hacking.qmd).

## Today

- Read & Discuss
    - [@simmons_false-positive_2011]
    - (Skim) [@John2012-tk]
    
---

Simmons, J. P., Nelson, L. D. & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. *Psychological Science*, *22*(11), 1359–1366. <https://doi.org/10.1177/0956797611417632>

## Central questions
    
- What are researcher 'degrees of freedom'?
- Why should we care about them?
- What are questionable research practices?
- Why should we care about them?

## False positives vs. false negatives

- Null hypothesis: There is no effect.
- Positive: What's actually true, a true fact
- Negative: What's actually untrue, a false fact
- What does our evidence say?

## Decision matrix    

| Evidence says | True fact | False fact |
|---------------|-----------|------------|
| True          | [True positive]{.green_assigned} | False positive (Type I) |
| False         | False negative (Type II) | [True negative]{.green_assigned} |

---

- Goals:
  - Minimize false positives (Type I errors) or maximize *specificity*
  - Minimize false negatives (Type II errors) or maximize *sensitivity*
- What we control
  - How to decide based on evidence
- What we don't control
  - What's true or false ($p(True fact)$)
  
## Biostatistics version

![https://www.scalestatistics.com/diagnostic-accuracy.html](https://www.scalestatistics.com/uploads/3/0/4/1/30413390/6343470_orig.jpg){fig-align="center" height="75%"}

## Practices
  
- Control alpha
  - $\alpha$
  - a probability or *p* value
  - to make false positive choices very rarely
- Control beta 
  - $\beta$
  - to make false negative choices very rarely
  - statistical 'power' is $1-\beta$.

---

::: {.callout-note}

$\alpha$, $\beta$, $\gamma$, $\delta$...

are letters from the Greek alphabet.

:::

---

| Evidence says | True fact | False fact |
|---------------|-----------|------------|
| True          | $1-\beta$ | $\alpha$   |
| False         | $\beta$   | $1-\alpha$  |

::: {.incremental}

- If fact is truly *true*, then our evidence should say so $1-\beta$ of the time
- If a fact is truly *false*, our evidence should say so $1-\alpha$ of the time.

:::

## What's a defensible choice? What's a questionable practice?

- In a paper, failing to report all of a study’s dependent measures
- Deciding whether to collect more data after looking to see whether the results were significant
- In a paper, failing to report all of a study’s conditions
- Stopping collecting data earlier than planned because one found the result that one had been looking for

---

- In a paper, “rounding off” a *p* value (e.g., reporting that a p value of .054 is less than .05)
- In a paper, selectively reporting studies that “worked”
- Deciding whether to exclude data after looking at the impact of doing so on the results
- In a paper, reporting an unexpected finding as having been predicted from the start
- In a paper, claiming that results are unaffected by demographic variables (e.g., gender) when one is actually unsure (or knows that they do)
- Falsifying data

---

::: {.callout-note}

Why might a researcher do these things?

Why might such choices be questionable?

:::

## Researcher 'degrees of freedom'

>The culprit is a construct we refer to as **researcher degrees of freedom**. In the course of collecting and analyzing data, researchers have many decisions to make: Should more data be collected? Should some observations be excluded? Which conditions should be combined and which ones compared? Which control variables should be considered? Should specific measures be combined or transformed or both?

---

::: {.callout-note}

What's a degree of freedom?

What does it mean in this context?

:::

---

>It is rare, and sometimes impractical, for researchers to make all these decisions beforehand. Rather, it is common (and accepted practice) for researchers to explore various analytic alternatives, to search for a combination that yields “statistical significance,” and to then report only what “worked.” 

---

::: {.callout-note}

- Why is it rare for researchers to make these decisions beforehand?
- Why is it impractical?
- What (relatively new) practice involves making many of these decisions in advance? [Hint](../schedule.html#monday-november-6).

:::

---

>The problem, of course, is that the likelihood of at least one (of many) analyses producing a falsely positive finding at the 5% level is necessarily greater than 5%.

<p style="text-align: center;">
[@simmons_false-positive_2011]
</p>

---

>This exploratory behavior is not the by-product of malicious intent, but rather the result of two factors: (a) ambiguity in how best to make these decisions and (b) the researcher’s desire to find a statistically significant result.

<p style="text-align: center;">
[@simmons_false-positive_2011]
</p>

---

::: {.callout-note}
What did [@Feynman1974-ld] say about when scientists should bend over backwards and why?
:::

---

:::: {.columns}

::: {.column width="40%"}
Richard Feynmann

![](https://upload.wikimedia.org/wikipedia/en/thumb/4/42/Richard_Feynman_Nobel.jpg/220px-Richard_Feynman_Nobel.jpg)

:::

::: {.column width="60%"}

> ...a specific, extra type of integrity that is not lying, but **bending over backwards to show how you’re maybe wrong**, that you ought to do when acting as a scientist. And this is our responsibility as scientists...and I think to laymen. 

:::

::::

---

:::: {.columns}

::: {.column width="40%"}
Richard Feynmann

![](https://upload.wikimedia.org/wikipedia/en/thumb/4/42/Richard_Feynman_Nobel.jpg/220px-Richard_Feynman_Nobel.jpg)

:::

::: {.column width="60%"}

> The first principle is that **you must not fool yourself—and you are the easiest person to fool**. So you have to be very careful about that. After you’ve not fooled yourself, it’s easy not to fool other scientists...

<p style="text-align:center;">
[@Feynman1974-ld]
</p>

:::

::::

## Choices to be made...

- Choosing dependent measure or measures
- Choosing sample size
  - Stopping early
- Choosing covariates 
- Reporting subsets of conditions
  - Trimming data (e.g., how to treat outliers)
  
## What's "too fast" for reaction time?
  
> In a perusal of roughly 30 *Psychological Science* articles, we discovered considerable inconsistency in, and hence considerable ambiguity about, this decision. Most (but not all) researchers excluded some responses for being too fast, but what constituted “too fast” varied enormously: the fastest 2.5%, or faster than 2 standard deviations from the mean, or faster than 100 or 150 or 200 or 300 ms. 

---

>Similarly, what constituted “too slow” varied enormously: the slowest 2.5% or 10%, or 2 or 2.5 or 3 standard deviations slower than the mean, or 1.5 standard deviations slower from that condition’s mean, or slower than 1,000 or 1,200 or 1,500 or 2,000 or 3,000 or 5,000 ms. None of these decisions is necessarily incorrect, but that fact makes any of them justifiable and hence potential fodder for self-serving justifications.

<p style="text-align: center;">
[@simmons_false-positive_2011]
</p>

---

![Table 1 from @Martinson2005-fk](../include/img/martinson-etal-2005-table-1.png){fig-align="center"}
## Study of 2,000 psychologists [@John2012-tk]

![Table 1 from @John2012-tk](../include/img/john-2012-table-01.png){fig-align="center"}

---

![Figure 1 from @John2012-tk](../include/img/john-2012-fig-01.png){fig-align="center"}

---

![Figure 2 from @John2012-tk](../include/img/john-2012-fig-02.png){fig-align="center"}

## Meta-analysis [@Xie2021-us]

![Self-reported prevalence; Figure 2 from @Xie2021-us](../include/img/xie-etal-2021-fig-2.png){fig-align="center"}
---

![Observed prevalence; Figure 3 from @Xie2021-us](../include/img/xie-etal-2021-fig-3.png){fig-align="center"}

## Your turn

- Which of the practices @John2012-tk focused on do *you* think are most questionable? Which the least questionable?
- Without discussing specifics, do you have first or second hand knowledge of any of these practices?
- Which of these practices are discouraged in research training (e.g. methods or stats courses, lab courses)? Which are overlooked? 
- Do the recommendations for authors and reviewers by [@simmons_false-positive_2011] address some of the issues?
- What other changes in researcher behavior would address the issues?

## Recommendations for authors

>1.	Authors must decide the rule for terminating data collection before data collection begins and report this rule in the article. Following this requirement may mean reporting the outcome of power calculations or disclosing arbitrary rules, such as “we decided to collect 100 observations” or “we decided to collect as many observations as we could before the end of the semester.” The rule itself is secondary, but it must be determined ex ante and be reported.

---

>2.	Authors must collect at least 20 observations per cell or else provide a compelling cost-of-data-collection justification. This requirement offers extra protection for the first requirement. Samples smaller than 20 per cell are simply not powerful enough to detect most effects, and so there is usually no good reason to decide in advance to collect such a small number of observations. Smaller samples, it follows, are much more likely to reflect interim data analysis and a flexible termination rule. In addition, as Figure 1 shows, larger minimum sample sizes can lessen the impact of violating Requirement 1.

---

>3.	Authors must list all variables collected in a study. This requirement prevents researchers from reporting only a convenient subset of the many measures that were collected, allowing readers and reviewers to easily identify possible researcher degrees of freedom. Because authors are required to just list those variables rather than describe them in detail, this requirement increases the length of an article by only a few words per otherwise shrouded variable. We encourage authors to begin the list with “only,” to assure readers that the list is exhaustive (e.g., “participants reported only their age and gender”).

---

>4.	Authors must report all experimental conditions, including failed manipulations. This requirement prevents authors from selectively choosing only to report the condition comparisons that yield results that are consistent with their hypothesis. As with the previous requirement, we encourage authors to include the word “only” (e.g., “participants were randomly assigned to one of only three conditions”).

---

>5.	If observations are eliminated, authors must also report what the statistical results are if those observations are included. This requirement makes transparent the extent to which a finding is reliant on the exclusion of observations, puts appropriate pressure on authors to justify the elimination of data, and encourages reviewers to explicitly consider whether such exclusions are warranted. Correctly interpreting a finding may require some data exclusions; this requirement is merely designed to draw attention to those results that hinge on ex post decisions about which data to exclude.

---

> 6.	If an analysis includes a covariate, authors must report the statistical results of the analysis without the covariate. Reporting covariate-free results makes transparent the extent to which a finding is reliant on the presence of a covariate, puts appropriate pressure on authors to justify the use of the covariate, and encourages reviewers to consider whether including it is warranted. Some findings may be persuasive even if covariates are required for their detection, but one should place greater scrutiny on results that do hinge on covariates despite random assignment.

<p style="text-align: center;">
[@simmons_false-positive_2011]
</p>

## For reviewers

>1.	Reviewers should ensure that authors follow the requirements. Review teams are the gatekeepers of the scientific community, and they should encourage authors not only to rule out alternative explanations, but also to more convincingly demonstrate that their findings are not due to chance alone. This means prioritizing transparency over tidiness; if a wonderful study is partially marred by a peculiar exclusion or an inconsistent condition, those imperfections should be retained. If reviewers require authors to follow these requirements, they will.

---

>2.	Reviewers should be more tolerant of imperfections in results. One reason researchers exploit researcher degrees of freedom is the unreasonable expectation we often impose as reviewers for every data pattern to be (significantly) as predicted. Underpowered studies with perfect results are the ones that should invite extra scrutiny.

---

>3.	Reviewers should require authors to demonstrate that their results do not hinge on arbitrary analytic decisions. Even if authors follow all of our guidelines, they will necessarily still face arbitrary decisions. For example, should they subtract the baseline measure of the dependent variable from the final result or should they use the baseline measure as a covariate? When there is no obviously correct way to answer questions like this, the reviewer should ask for alternatives. 

---

>For example, reviewer reports might include questions such as, “Do the results also hold if the baseline measure is instead used as a covariate?” Similarly, reviewers should ensure that arbitrary decisions are used consistently across studies (e.g., “Do the results hold for Study 3 if gender is entered as a covariate, as was done in Study 2?”).5 If a result holds only for one arbitrary specification, then everyone involved has learned a great deal about the robustness (or lack thereof) of the effect.

---

>4.	If justifications of data collection or analysis are not compelling, reviewers should require the authors to conduct an exact replication. If a reviewer is not persuaded by the justifications for a given researcher degree of freedom or the results from a robustness check, the reviewer should ask the author to conduct an exact replication of the study and its analysis. We realize that this is a costly solution, and it should be used selectively; however, “never” is too selective.

<p style="text-align:center;">
[@simmons_false-positive_2011]
</p>

---

John, L. K., Loewenstein, G. & Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. *Psychological Science*, *23*(5), 524–532. https://doi.org/10.1177/0956797611430953

---

>Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.

<p style="text-align:center;">
[@John2012-tk]
</p>

---

- Paper *not* openly accessible.
- Paper was accessible via PSU library.

## Results

```{r, fig.cap="Table 1 from [@John2012-tk](http://dx.doi.org/10.1177/0956797611430953)", out.width="80%"}
knitr::include_graphics("../include/img/john-2012-table-01.png")
```

>The two versions of the survey differed in the incentives they offered to respondents. In the Bayesian-truth-serum (BTS) condition, a scoring algorithm developed by one of the authors (Prelec, 2004) was used to provide incentives for truth telling. This algorithm uses respondents’ answers about their own behavior and their estimates of the sample distribution of answers as inputs in a truth-rewarding scoring formula. Because the survey was anonymous, compensation could not be directly linked to individual scores. Instead, respondents were told that we would make a donation to a charity of their choice, selected from five options, and that the size of this donation would depend on the truthfulness of their responses, as determined by the BTS scoring system. By inducing a (correct) belief that dishonesty would reduce donations, we hoped to amplify the moral stakes riding on each answer (for details on the donations, see Supplementary Results in the Supplemental Material). Respondents were not given the details of the scoring system but were told that it was based on an algorithm published in Science and were given a link to the article. There was no deception: Respondents’ BTS scores determined our contributions to the five charities. Respondents in the control condition were simply told that a charitable donation would be made on behalf of each respondent. (For details on the effect of the size of the incentive on response rates, see Participation Incentive Survey in the Supplemental Material.)

<p style="text-align:center;">
[@John2012-tk]
</p>

---

```{r, fig.cap="Figure 1 from [@John2012-tk](http://dx.doi.org/10.1177/0956797611430953)", out.width="80%"}
knitr::include_graphics("../include/img/john-2012-fig-01.png")
```

---

```{r, fig.cap="Figure 2 from [@John2012-tk](http://dx.doi.org/10.1177/0956797611430953)", out.width="80%"}
knitr::include_graphics("../include/img/john-2012-fig-02.png")
```

## Reproducibility notes for [@simmons_false-positive_2011]

- does not appear to have shared data or any supplementary materials along with the article.

## Reproducibility notes for [@John2012-tk]

>Supplemental Material
>Additional supporting information may be found at http://pss.sagepub.com/content/by/supplemental-data

<p style="text-align:center;">
[@John2012-tk]
</p>

---

- Visiting this URL (<http://pss.sagepub.com/content/by/supplemental-data>) takes one to the following:

```{r}
knitr::include_url("http://pss.sagepub.com/content/by/supplemental-data", height = 600)
```

---

- I went to the journal page and searched for the article title.
- Since the article is behind a paywall, I wasn't able to access the supplemental materials that way.
- After authenticating to the PSU library, I was able to find a PDF of the [supplementary material](https://psu.instructure.com/courses/2245007/files/folder/readings?preview=146306047). It and the [original paper](https://psu.instructure.com/courses/2245007/files/folder/readings?preview=146306050) are on Canvas.
- I was unable to find the raw data, but I found the questions on p. 5 of the supplementary material.

---

![Supplementary material for [@John2012-tk]](../include/img/john-etal-2012-items.png){fig-align="center"}

# Next time

*Work session: P-hacking*

- [Due]{.orange_due}
    - [Exercise 04: P-hack your way to scientific glory](exercises/ex04-p-hacking.qmd) write-up.
- Discuss [Exercise 04: P-hack your way to scientific glory](exercises/ex04-p-hacking.qmd).
- Work session
    - [Final project proposals](exercises/final-project.qmd), [due Friday, October 13]{.orange_due}.

# Resources

## References